{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook serves the purpose of cleaning the format of strings and numbers, removing missing values, and tokenization and normalization of paper abstract.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources consulted online**:\n",
    "1. https://stackoverflow.com/questions/40476680/how-to-use-tqdm-with-pandas-in-a-jupyter-notebook\n",
    "2. https://github.com/UChicago-CCA-2021/lucem_illud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lucem_illud\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helperd functions to do data cleaning and preprocessing of paper abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function that cleans the data into the format we want.\n",
    "def clean_data(load_file_path='../database/content_analysis.csv'):\n",
    "    '''\n",
    "    This function cleans a pandas DataFrame.\n",
    "    \n",
    "    Inputs:\n",
    "        load_file_path: file path to load original CSV file\n",
    "        save_file_path: file path to save cleaned CSV file\n",
    "\n",
    "    Returns: a cleaned DataFrame\n",
    "    '''\n",
    "    \n",
    "    # load the CSV file to a DataFrame\n",
    "    df = pd.read_csv(load_file_path, index_col=0)\n",
    "    \n",
    "    # rename columns\n",
    "    df.rename(columns={'abstract': 'award_abstract', 'url': 'author_url', 'total_citations': 'author_total_citations', 'Title': 'paper_title', \n",
    "                   'Year': 'publication_year', 'Cited by': 'paper_total_citations', 'Paper URL': 'paper_url', 'Authors': 'coauthors', \n",
    "                   'Publication Date': 'publication_date', 'Journal': 'journal', 'Abstract': 'paper_abstract', 'Citations': 'paper_yearly_citations'}, inplace=True)\n",
    "    \n",
    "    # drop columns \n",
    "    df.drop(['paper_url', 'publication_date', 'paper_yearly_citations', 'author_url', 'author_total_citations', 'h_index', 'interests', \n",
    "             'directorate', 'division', 'effective_date', 'expiration_date', 'award_title', 'award_abstract'], axis=1, inplace=True)\n",
    "\n",
    "    # reorder columns\n",
    "    df = df[['first_name', 'middle_name', 'last_name', 'email', 'institution', 'award_year', 'award_amount', 'paper_title', 'journal', \n",
    "             'publication_year', 'coauthors', 'paper_abstract', 'paper_total_citations', 'citation_2001', 'citation_2002', 'citation_2003', \n",
    "             'citation_2004', 'citation_2005', 'citation_2006', 'citation_2007', 'citation_2008', 'citation_2009', 'citation_2010', \n",
    "             'citation_2011', 'citation_2012', 'citation_2013', 'citation_2014', 'citation_2015', 'citation_2016', 'citation_2017', \n",
    "             'citation_2018', 'citation_2019', 'citation_2020', 'citation_2021', 'citation_2022', 'citation_2023', 'citation_2024']]\n",
    "\n",
    "    # convert email names to lowercase\n",
    "    df['email'] = df['email'].str.lower()\n",
    "    \n",
    "    # convert institution names to lowercase\n",
    "    df['institution'] = df['institution'].str.lower()\n",
    "    \n",
    "    # replace NaN values with empty lists in 'coauthors'\n",
    "    df['coauthors'] = df['coauthors'].apply(lambda x: [] if pd.isna(x) else x)\n",
    "\n",
    "    # convert each row in 'coauthors' to a list of authors\n",
    "    df['coauthors'] = df['coauthors'].apply(lambda x: x if isinstance(x, list) else x.split(', '))\n",
    "\n",
    "    # convert each author name to lowercase\n",
    "    df['coauthors'] = df['coauthors'].apply(lambda x: [name.lower() for name in x])\n",
    "\n",
    "    # replace NaN values with 'journal not found' in 'journal'\n",
    "    df['journal'] = df['journal'].apply(lambda x: 'journal not found' if pd.isna(x) else x)\n",
    "\n",
    "    # drop rows where 'abstract' is 'abstract not found' \n",
    "    df = df[df['paper_abstract'] != 'abstract not found']\n",
    "\n",
    "    # drop rows where 'abstract' has fewer than 20 words \n",
    "    df = df[df['paper_abstract'].astype(str).str.split().apply(len) >= 20]\n",
    "\n",
    "    # replace missing values in 'total_citation' with 0\n",
    "    df['paper_total_citations'] = df['paper_total_citations'].fillna(0) \n",
    "\n",
    "    # replace missing values in yearly citation columns with 0\n",
    "    for year in range(2001, 2025):\n",
    "        df[f'citation_{year}'] = df[f'citation_{year}'].fillna(0)\n",
    "\n",
    "    # assign data types to 'award_year' and 'publication_year'\n",
    "    df['award_year'] = pd.to_numeric(df['award_year'], errors='coerce')\n",
    "    df['award_year'] = df['award_year'].astype('int64').astype('category')\n",
    "    df['publication_year'] = pd.to_numeric(df['publication_year'], errors='coerce')\n",
    "    df['publication_year'] = df['publication_year'].astype('int64').astype('category')\n",
    "\n",
    "    # assign data types to the rest columns\n",
    "    columns = {f'citation_{year}': 'int64' for year in range(2001, 2025)}\n",
    "    df = df.astype({'first_name': 'object', 'middle_name': 'object', 'last_name': 'object', 'email': 'object', 'institution': 'category', \n",
    "                    'award_amount': 'int64', 'paper_title': 'object', 'journal': 'category', 'coauthors': 'object', 'paper_abstract': 'object', \n",
    "                    'paper_total_citations': 'int64'} | columns)\n",
    "\n",
    "    # reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # return the final cleaned DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(save_file_path='../database/preprocessed_content_analysis.csv'):\n",
    "    '''\n",
    "    This function preprocesses a pandas DataFrame.\n",
    "    \n",
    "    Inputs:\n",
    "        load_file_path: file path to load original CSV file\n",
    "        save_file_path: file path to save cleaned CSV file\n",
    "    '''\n",
    "    \n",
    "    # Get the cleanned DataFrame\n",
    "    df = clean_data()\n",
    "    \n",
    "    # tokenize 'title' column\n",
    "    df['tokenized_title'] = df['paper_title'].progress_apply(lambda x: [lucem_illud.word_tokenize(s) for s in lucem_illud.sent_tokenize(x)])\n",
    "    \n",
    "    # normalize 'tokenized_title' column\n",
    "    df['normalized_title'] = df['tokenized_title'].apply(lambda x: [lucem_illud.normalizeTokens(s) for s in x])\n",
    "    \n",
    "    # tokenize 'abstract' column\n",
    "    df['tokenized_abstract'] = df['paper_abstract'].progress_apply(lambda x: [lucem_illud.word_tokenize(s) for s in lucem_illud.sent_tokenize(x)])\n",
    "    \n",
    "    # nomalize 'tokenized_abstract' column\n",
    "    df['normalized_abstract'] = df['tokenized_abstract'].apply(lambda x: [lucem_illud.normalizeTokens(s) for s in x])\n",
    "\n",
    "    # save the dataframe to a csv file\n",
    "    df.to_csv(save_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the functions to clean and preprocess content_analysis dataframe (to get normalized tokens of abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160258/160258 [15:38<00:00, 170.81it/s]\n",
      "/Users/samcong/anaconda3/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "100%|██████████| 160258/160258 [1:45:45<00:00, 25.25it/s]  \n",
      "/Users/samcong/anaconda3/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "preprocess_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
